{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":90860,"databundleVersionId":10652987,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#splitting the dataset for training and validation functions \nfrom sklearn.model_selection import train_test_split\nfrom torchvision.datasets import ImageFolder\nimport shutil\nimport os\n\n#  original dataset directory\noriginal_train_dir = \"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/train\"\n\n# new directories for train and validation data splits\ntrain_dir = \"/kaggle/working/train_split\"\nval_dir = \"/kaggle/working/val_split\"\n\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(val_dir, exist_ok=True)\n\ndataset = ImageFolder(root=original_train_dir)\n\n# Split dataset into train and validation subsets\ntrain_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, stratify=dataset.targets)\n\n\nfor idx, (path, label) in enumerate(dataset.samples):\n    class_name = dataset.classes[label]\n    dest_dir = train_dir if idx in train_idx else val_dir\n    dest_class_dir = os.path.join(dest_dir, class_name)\n    os.makedirs(dest_class_dir, exist_ok=True)\n    shutil.copy2(path, dest_class_dir)\n\nprint(\"Train and validation datasets split successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#finetunnig and loading the pretrained resnet101 model \nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.optim.lr_scheduler import StepLR\nfrom pathlib import Path\nfrom torchvision.datasets.folder import default_loader\n\n#  data augmentation and preprocessing\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# a custom dataset for the test directory\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transform=None):\n        self.root = root\n        self.transform = transform\n        self.image_paths = list(Path(root).glob(\"*.jpg\"))  \n        self.loader = default_loader\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = self.loader(image_path)\n        if self.transform:\n            image = self.transform(image)\n        return image, str(image_path.name)  \n\n#  custom dataset for the test set\ntest_dataset = TestDataset(root=\"/kaggle/working/val_split\", transform=test_transforms)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Loading datasets\ntrain_dataset = datasets.ImageFolder(root=\"/kaggle/working/train_split\", transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Loading  a pre-trained ResNet model\nmodel = models.resnet101(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 50)  #  number of classes (50)\n\n# Move model to GPU if available else use CPU(which will take a lot of time to train)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nprint(device) #ensuring wether GPU is being used or not \n\n#  loss function and optimizer and learning rate scheduler \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4) #adamw for resnet\nscheduler = StepLR(optimizer, step_size=7, gamma=0.1)  #stepLR\n\n# Training Function\ndef train(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct += torch.sum(preds == labels.data)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    accuracy = correct.double() / len(train_loader.dataset)\n    return epoch_loss, accuracy\n\n# Validation function\ndef validate(model, test_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0  # Track total samples for accuracy calculation\n\n    with torch.no_grad():\n        for inputs, _ in test_loader: \n            inputs = inputs.to(device)\n\n            outputs = model(inputs)\n            running_loss += 0\n\n           \n            _, preds = torch.max(outputs, 1)\n            total += inputs.size(0)\n\n    epoch_loss = running_loss / total if total > 0 else 0\n    accuracy = 0  \n    return epoch_loss, accuracy\n\n\n# Train the model\nnum_epochs = 20 # 20 epcohs to train the model \nfor epoch in range(num_epochs):\n    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n    val_loss, val_accuracy = validate(model, test_loader, criterion, device)\n    \n    scheduler.step()\n\n\n    #printing the result after every epoch to see the progress of training loop\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss:.4f}  Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}  Validation Accuracy: {val_accuracy:.4f}\")\n\n# Saving the trained model\ntorch.save(model.state_dict(), \"resnet101_finetuned.pth\")\nprint(\"Model saved successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#ensemble learning and storing the final predicition.\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel, AutoModelForImageClassification, AutoImageProcessor\nimport pandas as pd\n\n# using gpu if available else cpu \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# loading all the files \ntrain_dir = \"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/train\"\ntest_dir = \"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/test\"\nclasses_path = \"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/classes.txt\"\n\n# Load class names (seen + unseen)\nwith open(classes_path) as f:\n    class_names = [line.strip() for line in f]\n\n# Data transformations\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\n# Dataset\nclass TestDataset(Dataset):\n    def __init__(self, test_dir, transform=None):\n        self.image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir)]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, os.path.basename(img_path)\n\ntest_dataset = TestDataset(test_dir, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Loading all the  Models for ensemble approach \nprint(\"Loading Models...\")\n\n# ResNet101\nresnet = models.resnet101(pretrained=True)\nresnet.fc = nn.Linear(resnet.fc.in_features, len(class_names))\nresnet.load_state_dict(torch.load(\"/kaggle/working/resnet101_finetuned.pth\"))\nresnet = resnet.to(device)\n\n# Vision Transformer\nvit_model = AutoModelForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\", num_labels=len(class_names)\n).to(device)\nvit_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\",use_fast=True)\n\n# CLIP Model\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Inference\nresnet.eval()\nvit_model.eval()\nclip_model.eval()\n\nprint(\"Performing Inference...\")\n\n# storing the predictions\nfinal_predictions = []\n\nwith torch.no_grad():\n    # Precompute text embeddings for all class names\n    text_inputs = clip_processor(text=class_names, return_tensors=\"pt\", padding=True).to(device)\n    text_features = clip_model.get_text_features(**text_inputs)\n\n    for images, image_names in test_loader:\n        images = images.to(device)\n\n        # Undo normalization for Vision Transformer\n        images_unnormalized = images * torch.tensor(std, device=device)[:, None, None] + \\\n                              torch.tensor(mean, device=device)[:, None, None]\n        images_unnormalized = torch.clamp(images_unnormalized, 0, 1)  # Ensure range [0, 1]\n\n        # ResNet predictions\n        resnet_outputs = resnet(images)\n        _, resnet_preds = torch.max(resnet_outputs, 1)\n\n        # Vision Transformer predictions\n        vit_inputs = vit_processor(images=list(images_unnormalized.cpu()), return_tensors=\"pt\").to(device)\n        vit_outputs = vit_model(**vit_inputs).logits\n        _, vit_preds = torch.max(vit_outputs, 1)\n\n        # CLIP Zero-Shot predictions\n        clip_image_features = clip_model.get_image_features(pixel_values=images)\n        cosine_sim = torch.nn.functional.cosine_similarity(\n            clip_image_features.unsqueeze(1), text_features.unsqueeze(0), dim=-1\n        )\n        _, clip_preds = torch.max(cosine_sim, dim=1)\n\n        # Ensemble learning : Weighted Voting\n        for name, resnet_pred, vit_pred, clip_pred in zip(image_names, resnet_preds, vit_preds, clip_preds):\n            # Adjusting weights based on model reliability(using the best set of weights tried after many combinations)\n            weights = {\n                resnet_pred.item(): 0.3,  # ResNet weight\n                vit_pred.item(): 0.3,    # Vision Transformer weight\n                clip_pred.item(): 0.4    # CLIP Zero-Shot weight\n            }\n            final_pred = max(weights, key=weights.get)  \n            final_predictions.append((name, class_names[final_pred])) #appending the final predictions\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Saving predictions with only class names (as per the format required by the challenge)\nsubmission_df = pd.DataFrame(final_predictions, columns=[\"image_id\", \"class\"])\nsubmission_df[\"class\"] = submission_df[\"class\"].str.split(\"\\t\").str[-1]  # Extract only the class name\nsubmission_df.to_csv(\"ensemble_submission.csv\", index=False)\nprint(\"Ensemble submission saved as 'ensemble_submission.csv'!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}